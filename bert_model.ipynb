{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67cf9d2-7346-4c55-9060-e244d090043f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3fe83cdc-86a3-4a77-aeeb-1fe321a5f014",
   "metadata": {},
   "source": [
    "# multi-Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b2c3221f-03ce-4d55-a5d7-452b356e6948",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "\n",
    "import random\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f6f9b8d2-3dfb-4bac-9245-00651631f4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module) :\n",
    "    \n",
    "    def forward(self, query, key, value, mask = None, dropout = None) :\n",
    "        scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(query.size(-1))\n",
    "        \n",
    "        if mask is not None :\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "            \n",
    "        p_attn = F.softmax(scores, dim = 1)\n",
    "        \n",
    "        if dropout is not None :\n",
    "            p_attn = dropout(p_attn)\n",
    "            \n",
    "        return torch.matmul(p_attn, value), p_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b300a97d-e780-4320-9668-d27259f1ca9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "437710f5-2fba-4b6f-ad65-7eece876737b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module) :\n",
    "    \n",
    "    def __init__(self, h, d_model, dropout = 0.1) :\n",
    "        super().__init__()\n",
    "        \n",
    "        assert d_model % h == 0\n",
    "        \n",
    "        self.d_k = d_model // h\n",
    "        self.h = h\n",
    "        \n",
    "        self.linear_layers = nn.ModuleList([nn.Linear(d_model, d_model) for _ in range(3)])\n",
    "        self.output_linear = nn.Linear(d_model, d_model)\n",
    "        self.attention = Attention()\n",
    "        self.dropout = nn.Dropout(p = dropout)\n",
    "        \n",
    "    def forward(self, query, key, value, mask = None):\n",
    "        batch_size = query.size()\n",
    "        query, key, value = [l(x).view(batch_size, -1, self.h, self.d_k).transpose(1,2) for l, x in zip(Self.linear_layers, (query, key, value))]\n",
    "        \n",
    "        x, attn = self.attention(query, key, value, mask = mask, dropout = self.dropout)\n",
    "        x = x.transpose(1,2).contiguous().view(batch_size, -1, self.h * self.d_k)\n",
    "        return self.output_linear(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ae35e1-4f40-43ab-8db5-104a5c63f1e2",
   "metadata": {},
   "source": [
    "# utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "743fbe44-631c-4ad5-9a0f-5e7acde69352",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, features, eps=1e-6):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.a_2 = nn.Parameter(torch.ones(features))\n",
    "        self.b_2 = nn.Parameter(torch.zeros(features))\n",
    "        self.eps = eps\n",
    "        \n",
    "    def forward(self, x):\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "848de35a-11ba-473f-9173-6a934b83905a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SublayerConnection(nn.Module):\n",
    "    def __init__(self, size, dropout):\n",
    "        super(SublayerConnection, self).__init__()\n",
    "        self.norm = LayerNorm(size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, sublayer):\n",
    "        return x + self.dropout(sublayer(self.norm(x)))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8a255e1e-4530-438b-9001-ddf672263edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GELU(nn.Module):\n",
    "     def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3f875da6-1dde-43cf-b9be-6c135b431928",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.w_1 = nn.Linear(d_model, d_ff)\n",
    "        self.w_2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.activation = GELU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.w_2(self.dropout(self.activation(self.w_1(X))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb58b39-c54c-4d64-953a-b61b8e76b18b",
   "metadata": {},
   "source": [
    "# transformer encoder block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "dff20c4c-fc78-429c-a46e-2d86e75941c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, hidden, attn_heads, feed_forward_hidden, dropout):\n",
    "        super().__init__()\n",
    "        self.attention = MultiHeadAttention(h = attn_heads, d_model=hidden)\n",
    "        self.feed_forward = PositionwiseFeedForward(d_model=hidden, d_ff = feed_forward_hidden, dropout = dropout)\n",
    "        self.input_sublayer = SublayerConnection(size=hidden, dropout=dropout)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        x = self.input_sublayer(x, lambda _x: self.attention.forward(_x, _x, _x, mask=mask))\n",
    "        x = self.output_sublayer(x, self.feed_forward)\n",
    "        return self.dropout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d15ac790-a57a-42d4-a31a-885a5ee5ed45",
   "metadata": {},
   "source": [
    "# Three Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d2ec0215-29fa-4f59-88b3-ab31b217f8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenEmbedding(nn.Embedding):\n",
    "    def __init__(self, vocab_size, embed_size = 512):\n",
    "        super().__init__(vocab_size, embed_size, padding_idx = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "af31fd65-b409-4c6f-966b-e2b5b7ae29bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegmentEmbedding(nn.Embedding):\n",
    "    def __init__(self, embed_size = 512):\n",
    "        super().__init__(3, embed_size, padding_idx = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7e3636ef-da80-415e-9baa-e9fa67d1abf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionEmbedding(nn.Module):\n",
    "    def __init__(self, d_model, max_len = 512):\n",
    "        super().__init__()\n",
    "        \n",
    "        pe = torch.zeros(max_len, d_model).float()\n",
    "        pe.required_grade = False\n",
    "        \n",
    "        position = torch.arange(0, max_len).float().unsqueeze(1)\n",
    "        div_term = (torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model)).exp()\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.pe[:, :x.size(1)]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "8b9ad7dc-eea8-4c5c-b2e5-a20b0040c638",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, dropout=0.1):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.token = TokenEmbedding(vocab_size=vocab_size, embed_size=embed_size)\n",
    "        self.position = PositionEmbedding(d_model = self.token.embedding_dim)\n",
    "        self.segment = SegmentEmbedding(embed_size=self.token.embedding_dim)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.embed_size = embed_size\n",
    "        \n",
    "    def forward(self, sequence, segment_label):\n",
    "        x = self.token(sequence) + self.position(sequence) + self.segment(segment_label)\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14198e2d-bb71-4901-93bc-d53686938cba",
   "metadata": {},
   "source": [
    "# BERT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "3123e2f5-402d-4781-810f-60280b894a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden = 768, n_layers = 12, attn_heads = 12, dropout = 0.1):\n",
    "        super().__init__()\n",
    "        self.hidden = hidden\n",
    "        self.n_layer = n_layers\n",
    "        self.attn_heads = attn_heads\n",
    "        self.feed_forward_hidden = hidden * 4\n",
    "        # bert임베딩 = token + segment + position\n",
    "        self. embedding = BERTEmbedding(vocab_size = vocab_size, embed_size = hidden)\n",
    "        #transformer 블럭\n",
    "        self.transformer_blocks = nn.ModuleList(\n",
    "            [TransformerBlock(hidden, attn_heads, hidden * 4, dropout) for _ in range(n_layers)]\n",
    "        )\n",
    "        \n",
    "    def forward(self, x, segment_info):\n",
    "        #attention masking\n",
    "        mask = (x>0).unsqueeze(1).repeat(1, x.size(1), 1).unsqueeze(1)\n",
    "        \n",
    "        x = self.embedding(x, segment_info)\n",
    "        \n",
    "        for transformer in self.transformer_blocks:\n",
    "            x = transformer.forward(x, mask)\n",
    "            \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5d73d2-f80c-4e76-9156-e05423821790",
   "metadata": {},
   "source": [
    "# MLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f1c7af0b-4f1e-4e3c-b188-944d3a096519",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedLanguageModel(nn.Module):\n",
    "    def __init__(self, hidden, vocab_size):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(hidden, vocab_size)\n",
    "        self.softmax = nn.LogSoftmax(dim = -1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.softmax(self.linear(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b649e116-f24d-428a-b22a-ab9eff4f84d3",
   "metadata": {},
   "source": [
    "# NSP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "22ed8335-b2bb-495f-8441-3faac07ebd48",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NextSentencePrediction(nn.Module):\n",
    "    def __init__(self, hidden):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(hidden, 2)\n",
    "        self.softmax = nn.LogSoftmax(dim = -1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.softmax(self.linear(x[:,0]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "8fa736ac-1b39-4435-8fe5-f6eda50bdbb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTLM(nn.Module):\n",
    "    def __init__(self, bert : BERT, vocab_size):\n",
    "        super().__init__()\n",
    "        self.bert = bert\n",
    "        self.next_sentence = NextSentencePrediction(self.bert.hidden)\n",
    "        self.mask_lm = MaskedLanguageModel(self.bert.hidden, vocab_size)\n",
    "        \n",
    "    def forward(self, x, segment_label):\n",
    "        x = self.bert(x, segment_label)\n",
    "        return self.next_sentence(x), self.mask_lm\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "31bbbbe7-e29b-4e87-9b1e-c96a33e4a06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "bert = BERT(30000)\n",
    "bertlm = BERTLM(bert,30000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "043c8644-2c1d-49ee-942f-17d4bb98f730",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERTLM(\n",
      "  (bert): BERT(\n",
      "    (embedding): BERTEmbedding(\n",
      "      (token): TokenEmbedding(30000, 768, padding_idx=0)\n",
      "      (position): PositionEmbedding()\n",
      "      (segment): SegmentEmbedding(3, 768, padding_idx=0)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (transformer_blocks): ModuleList(\n",
      "      (0-11): 12 x TransformerBlock(\n",
      "        (attention): MultiHeadAttention(\n",
      "          (linear_layers): ModuleList(\n",
      "            (0-2): 3 x Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (output_linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (attention): Attention()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (w_2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (activation): GELU()\n",
      "        )\n",
      "        (input_sublayer): SublayerConnection(\n",
      "          (norm): LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (next_sentence): NextSentencePrediction(\n",
      "    (linear): Linear(in_features=768, out_features=2, bias=True)\n",
      "    (softmax): LogSoftmax(dim=-1)\n",
      "  )\n",
      "  (mask_lm): MaskedLanguageModel(\n",
      "    (linear): Linear(in_features=768, out_features=30000, bias=True)\n",
      "    (softmax): LogSoftmax(dim=-1)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(bertlm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4820c25a-381c-4901-b181-d8e63d3f1687",
   "metadata": {},
   "source": [
    "# 모델학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c189566c-abb6-4c7c-9416-c3bb3db8f4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTTrainer:\n",
    "    def __init__(self, bert:BERT, vocab_size : int, train_dataloader : DataLoader, test_dataloader : DataLoader = None, lr : float = 1e-4, betas = (0.9, 0.999), weight_decay : float = 0.01, warmup_steps = 10000, with_cuda : bool = True, cuda_devices = None, log_freq : int = 10):\n",
    "        cuda_condition = torch.cuda.is_available() and with_cuda\n",
    "        self. device = torch.device(\"cuda:0\" if cuda_condition else \"cpu\")\n",
    "        \n",
    "        self.bert = bert\n",
    "        self.model = BERTLM(bert, vacab_size).to(self.device)\n",
    "        \n",
    "        self.train_data = train_dataloader\n",
    "        self.test_data = test_dataloader\n",
    "        \n",
    "        self.optim = Adam(self.model.parameters(), lr=lr, betas = betas, weight_decay = weight_decay)\n",
    "        self.optim_schedule = ScheduledOptim(self.optim, self.bert.hidden, n_warmup_steps=warmup_steps)\n",
    "        \n",
    "        self.criterion = nn.NLLoss(ignore_index=0)\n",
    "        self.log_freq = log_freq\n",
    "        \n",
    "        print(\"Total Parameters : \", sum([p.nelement() for p in self.model.parameters()]))\n",
    "        \n",
    "    def train(self, epoch):\n",
    "        self.iteration(epoch, self.train_data)\n",
    "    \n",
    "    def test(self, epoch):\n",
    "        self.iteration(epoch, self.test_data, train=False)\n",
    "    \n",
    "    def iteration(self, epoch, data_loader, train=True):\n",
    "        srt_code = \"train\" if train else \"test\"\n",
    "        data_iter = tqdm(enumerate(data_loader),desc=\"EP_%s:%d\" % (str_code, epoch),total=len(data_loader),bar_format=\"{l_bar}{r_bar}\")\n",
    "        \n",
    "        avg_loss = 0.0\n",
    "        total_correct = 0\n",
    "        total_element = 0\n",
    "        \n",
    "        for i in data_iter:\n",
    "            data = {key: value.to(self.device) for key, value in data.items()}\n",
    "            next_sent_output, mask_lm_output = self.model.forward(data[\"bert_input\"], data[\"segment_label\"])\n",
    "            next_loss = self.criterion(text_sent_output, data[\"is_next\"])\n",
    "            mask_loss = self.criterion(mask_lm_output.transpose(1,2), data[\"bert_label\"])\n",
    "            loss = next_loss + mask_loss\n",
    "            \n",
    "            if train:\n",
    "                self.optim_schedule.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optim_schedule.step_and_update_lr()\n",
    "                \n",
    "            correct = next_sent_output.argmax(dim=1).eq(data[\"is_next\"]).sum().item()\n",
    "            avg_loss += loss.item()\n",
    "            total_correct += correct\n",
    "            total_element += data[\"is_next\"].nelement()\n",
    "            \n",
    "            post_fix = {\n",
    "                \"epoch\": epoch,\n",
    "                \"iter\" : i,\n",
    "                \"avg_loss\" : avg_loss / (i+1),\n",
    "                \"avg_acc\" : total_correct / total_element * 100\n",
    "                \"loss\" : loss.item()\n",
    "            }\n",
    "            \n",
    "            if i% self.log_freq == 0:\n",
    "                data_iter.write(str(post_fix))\n",
    "        print(\"EP%d_%s, avg_loss=\" % (epoch, str_code), avg_loss / len(data_iter), \"total_acc=\",\n",
    "            total_correct * 100.0 / total_element)\n",
    "\n",
    "        \n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ragk",
   "language": "python",
   "name": "ragk"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
