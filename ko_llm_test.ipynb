{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dhkim/anaconda3/envs/llama_dhk/lib/python3.9/site-packages/torch/cuda/__init__.py:611: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n",
      "/home/dhkim/anaconda3/envs/llama_dhk/lib/python3.9/site-packages/trl/trainer/ppo_config.py:141: UserWarning: The `optimize_cuda_cache` arguement will be deprecated soon, please use `optimize_device_cache` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, HfArgumentParser, TrainingArguments, pipeline, logging, TextStreamer\n",
    "from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training, get_peft_model\n",
    "import os,torch, wandb, platform, warnings\n",
    "import gradio \n",
    "from datasets import load_dataset\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "메모리 사용량/총 메모리: 0.0GB / 0.0GB\n"
     ]
    }
   ],
   "source": [
    " print('메모리 사용량/총 메모리: ' + str(torch.cuda.memory_allocated('cuda:0')/1024**3 ) + 'GB / ' + str(torch.cuda.memory_reserved('cuda:0')/1024**3 ) + 'GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available: True\n",
      "Number of CUDA devices: 1\n",
      "--- CUDA Device 0 ---\n",
      "Name: NVIDIA RTX A6000\n",
      "Compute Capability: (8, 6)\n",
      "Total Memory: 51041271808 bytes\n",
      "--- CPU Information ---\n",
      "Processor: x86_64\n",
      "System: Linux 6.2.0-34-generic\n",
      "Python Version: 3.9.12\n"
     ]
    }
   ],
   "source": [
    "def print_system_specs():\n",
    "    # Check if CUDA is available\n",
    "    is_cuda_available = torch.cuda.is_available()\n",
    "    print(\"CUDA Available:\", is_cuda_available)\n",
    "# Get the number of available CUDA devices\n",
    "    num_cuda_devices = torch.cuda.device_count()\n",
    "    print(\"Number of CUDA devices:\", num_cuda_devices)\n",
    "    if is_cuda_available:\n",
    "        for i in range(num_cuda_devices):\n",
    "            # Get CUDA device properties\n",
    "            device = torch.device('cuda', i)\n",
    "            print(f\"--- CUDA Device {i} ---\")\n",
    "            print(\"Name:\", torch.cuda.get_device_name(i))\n",
    "            print(\"Compute Capability:\", torch.cuda.get_device_capability(i))\n",
    "            print(\"Total Memory:\", torch.cuda.get_device_properties(i).total_memory, \"bytes\")\n",
    "    # Get CPU information\n",
    "    print(\"--- CPU Information ---\")\n",
    "    print(\"Processor:\", platform.processor())\n",
    "    print(\"System:\", platform.system(), platform.release())\n",
    "    print(\"Python Version:\", platform.python_version())\n",
    "print_system_specs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
      "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
      "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
      "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
      "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
      "\n",
      "    A token is already saved on your machine. Run `huggingface-cli whoami` to get more information or `huggingface-cli logout` if you want to log out.\n",
      "    Setting a new token will erase the existing one.\n",
      "    To login, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
      "Token: ········\n",
      "Add token as git credential? (Y/n) y\n",
      "Token is valid (permission: write).\n",
      "\u001b[1m\u001b[31mCannot authenticate through git-credential as no helper is defined on your machine.\n",
      "You might have to re-authenticate when pushing to the Hugging Face Hub.\n",
      "Run the following command in your terminal in case you want to set the 'store' credential helper as default.\n",
      "\n",
      "git config --global credential.helper store\n",
      "\n",
      "Read https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage for more details.\u001b[0m\n",
      "Token has not been saved to git credential helper.\n",
      "Your token has been saved to /home/dhkim/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import interpreter_login\n",
    "\n",
    "interpreter_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23fda959da004fe4b903f71e323e5787",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MistralForCausalLM(\n",
       "  (model): MistralModel(\n",
       "    (embed_tokens): Embedding(32002, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x MistralDecoderLayer(\n",
       "        (self_attn): MistralAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): MistralRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): MistralMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): MistralRMSNorm()\n",
       "        (post_attention_layernorm): MistralRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): MistralRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32002, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Minirecord/Mini_synatra_7b_02\")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"Minirecord/Mini_synatra_7b_02\")\n",
    "model.to('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream(user_prompt):\n",
    "    runtimeFlag = \"cuda:0\"\n",
    "    system_prompt = \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\"\n",
    "    B_INST, E_INST = \"### Instruction:\", \"### Response:\"\n",
    "\n",
    "    prompt = f\"{system_prompt}{B_INST}{user_prompt.strip()}{E_INST}\"\n",
    "\n",
    "    inputs = tokenizer([prompt], return_tensors=\"pt\").to(runtimeFlag)\n",
    "\n",
    "    # Generate response\n",
    "    outputs = model.generate(**inputs, max_new_tokens=1500)\n",
    "\n",
    "    # Decode and print the generated response\n",
    "    print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dhkim/anaconda3/envs/llama_dhk/lib/python3.9/site-packages/transformers/generation/utils.py:1517: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> Below is an instruction that describes a task. Write a response that appropriately completes the request.### Instruction:나 행복해### Response:> 안녕하세요! 저는 인공지능 언어 모델이기 때문에 개인적인 감정이나 감정을 가지고 있지 않습니다. 하지만 여러분의 기분을 밝게 해드리기 위해 여기 있습니다. 무엇이든 자유롭게 공유하고 궁금한 점이 있으면 언제든지 문의해 주세요. 저는 여기 있어서 여러분을 돕고 즐거운 시간을 보내기 위해 여기 있습니다.</s>\n"
     ]
    }
   ],
   "source": [
    "stream(\"나 행복해\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> Below is an instruction that describes a task. Write a response that appropriately completes the request.### Instruction:나 오늘 떡볶이 먹어서 기뻐### Response: 떡볶이는 맛있는 요리입니다! 떡볶이를 먹을 때 기분이 좋다는 것은 정상입니다. 떡볶이는 맛있고 즐거운 식사를 즐기는 것과 같습니다. 떡볶이를 먹을 때 기분이 좋다면 떡볶이를 먹을 때 기분이 좋다는 것은 떡볶이를 먹을 때 기분이 좋다는 것입니다. 떡볶이를 먹을 때 기분이 좋다는 것은 떡볶이를 먹을 때 기분이 좋다는 것입니다. 떡볶이를 먹을 때 기분이 좋다는 것은 떡볶이를 먹을 때 기분이 좋다는 것입니다. 떡볶이를 먹을 때 기분이 좋다는 것은 떡볶이를 먹을 때 기분이 좋다는 것입니다. 떡볶이를 먹을 때 기분이 좋다는 것은 떡볶이를 먹을 때 기분이 좋다는 것입니다. 떡볶이를 먹을 때 기분이 좋다는 것은 떡볶이를 먹을 때 기분이 좋다는 것입니다. 떡볶이를 먹을 때 기분이 좋다는 것은 떡볶이를 먹을 때 기분이 좋다는 것입니다. 떡볶이를 먹을 때 기분이 좋다는 것은 떡볶이를 먹을 때 기분이 좋다는 것입니다. 떡볶이를 먹을 때 기분이 좋다는 것은 떡볶이를 먹을 때 기분이 좋다는 것입니다. 떡볶이를 먹을 때 기분이 좋다는 것은 떡볶이를 먹을 때 기분이 좋다는 것입니다. 떡볶이를 먹을 때 기분이 좋다는 것은 떡볶이를 먹을 때 기분이 좋다는 것입니다. 떡볶이를 먹을 때 기분이 좋다는 것은 떡볶이를 먹을 때 기분이 좋다는 것입니다. 떡볶이를 먹을 때 기분이 좋다는 것은 떡볶이를 먹을 때 기분이 좋다는 것입니다. 떡볶이를 먹을 때 기분이 좋다는 것은 떡볶이를 먹을 때 기분이 좋다는 것입니다. 떡볶이를 먹을 때 기분이 좋다는 것은 떡볶이를 먹을 때 기분이 좋다는 것입니다. 떡볶이를 먹을 때 기분이 좋다는 것은 떡볶이를 먹을 때 기분이 좋다는 것입니다. 떡볶이를 먹을 때 기분이 좋다는 것은 떡볶이를 먹을 때 기분이 좋다는 것입니다. 떡볶이를 먹을 때 기분이 좋다는 것은 떡볶이를 먹을 때 기분이 좋다는 것입니다. 떡볶이를 먹을 때 기분이 좋다는 것은 떡볶이를 먹을 때 기분이 좋다는 것입니다. 떡볶이를 먹을 때 기분이 좋다는 것은 떡볶이를 먹을 때 기분이 좋다는 것입니다. 떡볶이를 먹을 때 기분이 좋다는 것은 떡볶이를 먹을 때 기분이 좋다는 것입니다. 떡볶이를 먹을 때 기분이 좋다는 것은 떡볶이를 먹을 때 기분이 좋다는 것입니다. 떡볶이를 먹을 때 기분이 좋다는 것은 떡볶이를 먹을 때 기분이 좋다는 것입니다. 떡볶이를 먹을 때 기분이 좋다는 것은 떡볶이를 먹을 때 기분이 좋다는 것입니다. 떡볶이를 ��\n"
     ]
    }
   ],
   "source": [
    "stream(\"나 오늘 떡볶이 먹어서 기뻐\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama_dhk",
   "language": "python",
   "name": "llama_dhk"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
